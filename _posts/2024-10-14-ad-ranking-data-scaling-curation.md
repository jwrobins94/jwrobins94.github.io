---
layout: post
title: "Data Curation and Scaling in Ad Ranking"
date: 2024-10-14
categories: blog
---

## Background

"Data" has received a lot of attention over the last few years in the language modeling space, and much progress has been made in answering questions such as:
1. How do we curate the right dataset for our use case?
2. How does model quality scale with data volume?
3. How can we take an existing model and improve it using domain-specific data?

As a few examples:
1. [​​A Pretrainer's Guide to Training Data](https://arxiv.org/abs/2305.13169), [RedPajama-Data-v2](https://www.together.ai/blog/redpajama-data-v2), [RefinedWeb](https://arxiv.org/pdf/2306.01116), [The Stack v2](https://arxiv.org/pdf/2402.19173), and [Phi-3](https://arxiv.org/abs/2404.14219) discuss data acquisition, filtering, annotation, and deduplication in the context of language model pretraining.
2. [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361), [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556), [Scaling Language Models: Methods, Analysis & Insights from Training Gopher](https://arxiv.org/abs/2112.11446), and [The Llama 3 Herd of Models](https://arxiv.org/abs/2407.21783) discuss scaling laws that aim to predict how loss and benchmark performance scale with increasing model size and data volume.
3. [Code Llama](https://arxiv.org/abs/2308.12950), [The Llama 3 Herd of Models](https://arxiv.org/abs/2407.21783), and [AutoIF](https://arxiv.org/abs/2406.13542) discuss how LLMs can be improved through continued pretraining, annealing, and fine-tuning (respectively) on high-quality domain-specific data sources.

In contrast, "data" is an underexplored topic in ad ranking research.

Meta attempted to replicate some of the earlier scaling law results in [Understanding Scaling Laws for Recommendation Models](https://arxiv.org/abs/2208.08489), concluding that common [DLRM](https://ai.meta.com/blog/dlrm-an-advanced-open-source-deep-learning-recommendation-model/)-style click prediction models in the industry are already too large to benefit from additional parameter scaling; instead, data scaling is the more immediate path forward towards better models. However, additional data is hard to come by in ads as these models typically train on ad impressions generated by their respective platforms.

Pinterest lightly discussed data quality in [Practical Lessons from Conversion Ads at Pinterest](https://aiconference.com/wp-content/uploads/2023/10/Aayush-Mudgal-Practical-Lessons-from-Conversion-Modeling-on-Pinterest-Ads_-AI-Conference_Aayush.pptx-1.pdf), however they provide little quantitative insight into the implications of their filtering on model quality.

## Data in Ad Ranking

Below are a few questions that seem to be poorly answered in available literature on ad ranking systems:
1. How does model quality improve as we accumulate more data for a given advertiser or vertical?
2. How do misconfigured data integrations and spam affect model quality for other advertisers?
3. What is the impact of (nearly) duplicate impressions in the training data?

These are expanded on in the sections below.

### Model Quality vs Advertiser Data Volume

While [Understanding Scaling Laws for Recommendation Models](https://arxiv.org/abs/2208.08489) explored how ranking model quality scales with total data volume, the total amount of available data is relatively fixed in real-world ad ranking systems. Hence, it is challenging to leverage such results to deliver a better model. That said, it is often possible to influence data volume in small but significant data segments, e.g. encouraging a particular advertiser to increase their advertising budget or conducting a targeted outreach to advertisers in a specific vertical.

To take a narrower focus: a common question that ad ranking teams face is “how much data do we need from this advertiser for the model to learn?” The answer to this question has important implications on which products we recommend to that advertiser, how much we recommend they spend, whether additional marketplace levers should be employed, and how long the advertiser should wait before they should expect to meet their KPIs.

Let's break this down.

**"how much data do we need"**

There are a few dimensions by which we can think about this:
1. Total accumulated data volume
2. Rate of data accumulation (~= fraction of dataset from this advertiser)
3. Signal per unit of data volume (e.g. click-through rate)

It is reasonable to expect that a conversion prediction model performs best for advertisers who receive lots of impressions per day, have a high conversion rate (e.g. lots of clicks), and who have been in such a state for a long time.

Note that while all these are _mostly_ in control of the advertiser, the ads platform does have some influence on the portion of the model’s training data that comes from a given advertiser. This is because typical ad ranking models are trained on sampled data, e.g. negative sampling as popularized by [Facebook](https://scontent-sjc3-1.xx.fbcdn.net/v/t39.8562-6/240842589_204052295113548_74168590424110542_n.pdf?_nc_cat=109&ccb=1-7&_nc_sid=e280be&_nc_ohc=erGPWmR0umkQ7kNvgEQOmTm&_nc_ht=scontent-sjc3-1.xx&_nc_gid=Aq-X5xEY5iIqUaDZ0-z7tnk&oh=00_AYBdVvt98FbeJPhI8dJxkf2H63Y4Dim77HBQ4eKHt5xGHA&oe=67134E8A). The sampling rate is usually constant across advertisers, but it doesn’t have to be that way.

**"from this advertiser"**

We are rarely in a "true" cold start scenario for advertisers. When a new advertiser joins an advertising platform, it’s likely that there already exist other advertisers like them on the platform (e.g. same industry, advertising similar products). Existing advertisers will also frequently start new ad campaigns, but again, these campaigns aren’t 100% new – the conversion patterns that the model has learned from that advertiser’s other campaigns should still be useful.

How all of these factors come together to influence model quality for a particular advertiser is not well-documented and is an interesting direction for future exploration.

### Label Quality Implications

Conversion prediction systems such as those at [Snap](https://eng.snap.com/machine-learning-snap-ad-ranking?lang=en-US) or [Pinterest](https://aiconference.com/wp-content/uploads/2023/10/Aayush-Mudgal-Practical-Lessons-from-Conversion-Modeling-on-Pinterest-Ads_-AI-Conference_Aayush.pptx-1.pdf) often rely on advertiser-supplied conversion events to report a user’s purchases, page views, etc. to the advertising platform. For each conversion event, the ads platform attempts to find a recent ad impression for the user that they can credit for "causing" the conversion.

A challenge with such a system is data quality, as the advertising platform is dependent on external advertisers to truthfully report conversion events. It is common for advertisers to mislabel event types (e.g. calling a page view a “purchase”), send duplicate conversions, or send conversion events on a significantly delayed cadence. Even for on-platform events like "clicks", label quality is not guaranteed; users may accidentally click or engage with clickbait such that their click interactions are not representative of their true interests across advertisers.

Since a typical ad ranking model will serve _many_ advertisers, it is theoretically possible for one advertiser’s data quality to influence ranking performance for other advertisers who share that same model.

In ads, there is little available research that studies this behavior. That said, the success of language models like [Phi-3](https://arxiv.org/abs/2404.14219), which prioritize training on a smaller amount of high-quality data, begs the question as to whether similar properties may hold in ad ranking models.

### Ad Repetition and Data Duplication

You're likely familiar with the experience of seeing the same ad repeatedly on an ad platform. When it comes to recommendation systems, this phenomenon is relatively unique to ad ranking: you typically watch new movies and TV shows on netflix, new shorts on TikTok, or consume new tweets on X. However, it’s not uncommon for you to consume the same or similar ad over and over.

From the model's perspective, many of these ad impressions look near-identical:
1. Same user
2. Same ad
3. Similar context
4. Same outcome?

Although the user, ad, and context features will change slightly over time, the feature vectors for these "same user, same ad, similar time" impressions are mostly the same. Additionally, due to the low conversion rate in typical click-prediction and conversion-prediction models, the vast majority of these impressions will share a negative label.

In language modeling, it is standard practice to perform both exact deduplication and soft deduplication on training data. However, this is not the case in ad ranking models. The most common _implicit protection_ against data duplication is negative sampling (popularized by [Practical Lessons from Predicting Clicks on Ads at Facebook](https://scontent-sjc3-1.xx.fbcdn.net/v/t39.8562-6/240842589_204052295113548_74168590424110542_n.pdf?_nc_cat=109&ccb=1-7&_nc_sid=e280be&_nc_ohc=erGPWmR0umkQ7kNvgEQOmTm&_nc_ht=scontent-sjc3-1.xx&_nc_gid=Aq-X5xEY5iIqUaDZ0-z7tnk&oh=00_AYBdVvt98FbeJPhI8dJxkf2H63Y4Dim77HBQ4eKHt5xGHA&oe=67134E8A)) in which a random subset of negative samples are thrown out at dataset generation time. However, negative sampling has the effect of uniformly throwing out negative samples. As a result, a large number of unique ad-user samples will also be discarded.

It remains to be seen whether there are better sampling methods available than standard negative sampling, both to reduce the impact of duplicate data (if there is any) and to increase the diversity of training data.